# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Massachusetts Library of Things - a unified search app for borrowable non-traditional items (tools, technology, outdoor gear) across Massachusetts public library networks.

## Common Commands

```bash
# Development
npm install          # Install dependencies
npm run dev          # Start dev server (http://localhost:5173)
npm run build        # Production build to dist/

# Data Management
python scripts/consolidate.py           # Merge all network JSON into all_networks.json
python scripts/validate_data.py         # Validate JSON data files
python scripts/scrape_all.py            # Run web scrapers for all networks
python scripts/scrape_all.py -n minuteman  # Scrape single network
python scripts/generate_xlsx.py --all   # Generate Excel files

# Testing
python scripts/validate_data.py --strict  # Fail on warnings
```

## Architecture

### Frontend (React + Vite + Tailwind)
- `src/App.jsx` - Main component, loads `/data/all_networks.json`, manages search/filter state
- `src/components/` - UI components (Header, SearchBar, FilterPanel, ItemCard, ItemList, Footer, Disclaimer)
- `src/utils/catalogUrls.js` - Generates catalog URLs for different library systems (Evergreen, Aspen Discovery, SirsiDynix, Polaris, BiblioCommons)
- `src/utils/categories.js` - Category icons and normalization

### Data Layer
- `data/{network}_items.json` - Per-network JSON files (minuteman, cwmars, sails, mbln)
- `data/all_networks.json` - Consolidated file generated by `consolidate.py`
- `public/data/all_networks.json` - Copy for build, loaded at runtime by App.jsx

### Scraping Infrastructure (Python)
- `scripts/scrapers/base_scraper.py` - Abstract base with rate limiting, retry logic
- `scripts/scrapers/libguides_scraper.py` - Parses LibGuides HTML (most libraries use this)
- `scripts/scrapers/config.py` - Network URLs, category mappings, scrape settings

### CI/CD (GitHub Actions)
- `.github/workflows/deploy.yml` - Builds and deploys to Netlify on push to main
- `.github/workflows/update-data.yml` - Weekly scrape (Sundays 3AM EST), commits data changes

## Key Design Decisions

1. **Static site with build-time data** - Data in `public/data/` is fetched client-side, no backend needed
2. **Network-agnostic item schema** - Each item has `network` field; app filters by network
3. **Catalog URL generation** - Since most items don't have direct catalog IDs, we generate search URLs
4. **Independence disclaimer** - Must be prominently displayed; project is NOT affiliated with libraries

## Adding a New Library Network

1. Create `data/{network}_items.json` with structure:
```json
{
  "network": { "name": "...", "short_name": "...", "region": "..." },
  "libraries": [{ "name": "...", "location": "...", "lot_url": "..." }],
  "items": [{ "library": "...", "category": "...", "name": "...", "description": "..." }],
  "metadata": { "last_updated": "YYYY-MM-DD" }
}
```
2. Add network to `NETWORK_FILES` dict in `scripts/consolidate.py`
3. Add network metadata to `NETWORK_METADATA` in `scripts/consolidate.py`
4. Add catalog config to `networkCatalogs` in `src/utils/catalogUrls.js`
5. Add library sources to `NETWORK_SOURCES` in `scripts/scrapers/config.py`
6. Run `python scripts/consolidate.py`

## Standard Categories

Use these for consistency:
- Home Improvement, Measurement & Detection, Home Inspection
- Auto/Vehicle, Bicycle, Gardening
- Outdoor/Camping, Outdoor Games
- Crafts/Sewing, Technology, Technology/Gaming
- Music, Kitchen, Party/Event
- Wellness/Health, Games, Science/Education

## Deployment

Configured for Netlify. Required GitHub secrets:
- `NETLIFY_AUTH_TOKEN` - Personal access token
- `NETLIFY_SITE_ID` - From Netlify dashboard
- `NETLIFY_BUILD_HOOK` - Optional, for triggering deploys after data updates
